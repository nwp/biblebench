{
  "grok-4-1-fast": {
    "id": "grok-4-1-fast",
    "openrouterId": "x-ai/grok-4.1-fast",
    "displayName": "Grok 4.1 Fast",
    "provider": "X.AI",
    "description": "Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window.\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
    "contextLength": 2000000,
    "cost": {
      "prompt": 2e-7,
      "completion": 5e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "mistral-large-2512": {
    "id": "mistral-large-2512",
    "openrouterId": "mistralai/mistral-large-2512",
    "displayName": "Mistral Large 2512",
    "provider": "Mistral",
    "description": "Mistral Large 3 2512 is Mistral’s most capable model to date, featuring a sparse mixture-of-experts architecture with 41B active parameters (675B total), and released under the Apache 2.0 license.",
    "contextLength": 262144,
    "cost": {
      "prompt": 5e-7,
      "completion": 0.0000015,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gpt-5-nano": {
    "id": "gpt-5-nano",
    "openrouterId": "openai/gpt-5-nano",
    "displayName": "GPT-5 Nano",
    "provider": "OpenAI",
    "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
    "contextLength": 400000,
    "cost": {
      "prompt": 5e-8,
      "completion": 4e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image",
        "file"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "claude-sonnet-4-5": {
    "id": "claude-sonnet-4-5",
    "openrouterId": "anthropic/claude-sonnet-4.5",
    "displayName": "Claude Sonnet 4.5",
    "provider": "Anthropic",
    "description": "Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.",
    "contextLength": 1000000,
    "cost": {
      "prompt": 0.000003,
      "completion": 0.000015,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image",
        "file"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "claude-opus-4-5": {
    "id": "claude-opus-4-5",
    "openrouterId": "anthropic/claude-opus-4.5",
    "displayName": "Claude Opus 4.5",
    "provider": "Anthropic",
    "description": "Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.",
    "contextLength": 200000,
    "cost": {
      "prompt": 0.000005,
      "completion": 0.000025,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "file",
        "image",
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "deepseek-v3-2": {
    "id": "deepseek-v3-2",
    "openrouterId": "deepseek/deepseek-v3.2",
    "displayName": "DeepSeek V3.2",
    "provider": "DeepSeek",
    "description": "DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
    "contextLength": 163840,
    "cost": {
      "prompt": 2.5e-7,
      "completion": 3.8e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "grok-4": {
    "id": "grok-4",
    "openrouterId": "x-ai/grok-4",
    "displayName": "Grok 4",
    "provider": "X.AI",
    "description": "Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)",
    "contextLength": 256000,
    "cost": {
      "prompt": 0.000003,
      "completion": 0.000015,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "image",
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gpt-5-2": {
    "id": "gpt-5-2",
    "openrouterId": "openai/gpt-5.2",
    "displayName": "GPT-5.2",
    "provider": "OpenAI",
    "description": "GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long context perfomance compared to GPT-5.1. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks.\n\nBuilt for broad task coverage, GPT-5.2 delivers consistent gains across math, coding, sciende, and tool calling workloads, with more coherent long-form answers and improved tool-use reliability.",
    "contextLength": 400000,
    "cost": {
      "prompt": 0.00000175,
      "completion": 0.000014,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "file",
        "image",
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "glm-4-7": {
    "id": "glm-4-7",
    "openrouterId": "z-ai/glm-4.7",
    "displayName": "GLM-4.7",
    "provider": "Zhipu AI",
    "description": "GLM-4.7 is Z.AI’s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics.",
    "contextLength": 202752,
    "cost": {
      "prompt": 4e-7,
      "completion": 0.0000015,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gemini-3-pro-preview": {
    "id": "gemini-3-pro-preview",
    "openrouterId": "google/gemini-3-pro-preview",
    "displayName": "Gemini 3 Pro Preview",
    "provider": "Google",
    "description": "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.",
    "contextLength": 1048576,
    "cost": {
      "prompt": 0.000002,
      "completion": 0.000012,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gemini-3-flash-preview": {
    "id": "gemini-3-flash-preview",
    "openrouterId": "google/gemini-3-flash-preview",
    "displayName": "Gemini 3 Flash Preview",
    "provider": "Google",
    "description": "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\n\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models.",
    "contextLength": 1048576,
    "cost": {
      "prompt": 5e-7,
      "completion": 0.000003,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "minimax-m2-1": {
    "id": "minimax-m2-1",
    "openrouterId": "minimax/minimax-m2.1",
    "displayName": "MiniMax M2.1",
    "provider": "MiniMax",
    "description": "MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic workflows, and modern application development. With only 10 billion activated parameters, it delivers a major jump in real-world capability while maintaining exceptional latency, scalability, and cost efficiency.\n\nCompared to its predecessor, M2.1 delivers cleaner, more concise outputs and faster perceived response times. It shows leading multilingual coding performance across major systems and application languages, achieving 49.4% on Multi-SWE-Bench and 72.5% on SWE-Bench Multilingual, and serves as a versatile agent “brain” for IDEs, coding tools, and general-purpose assistance.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).",
    "contextLength": 204800,
    "cost": {
      "prompt": 3e-7,
      "completion": 0.0000012,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "llama-4-maverick": {
    "id": "llama-4-maverick",
    "openrouterId": "meta-llama/llama-4-maverick",
    "displayName": "Llama 4 Maverick",
    "provider": "Meta",
    "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
    "contextLength": 1048576,
    "cost": {
      "prompt": 1.5e-7,
      "completion": 6e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text",
        "image"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "intellect-3": {
    "id": "intellect-3",
    "openrouterId": "prime-intellect/intellect-3",
    "displayName": "Intellect-3",
    "provider": "Prime Intellect",
    "description": "INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-trained from GLM-4.5-Air-Base using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). It offers state-of-the-art performance for its size across math, code, science, and general reasoning, consistently outperforming many larger frontier models. Designed for strong multi-step problem solving, it maintains high accuracy on structured tasks while remaining efficient at inference thanks to its MoE architecture.",
    "contextLength": 131072,
    "cost": {
      "prompt": 2e-7,
      "completion": 0.0000011,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "claude-haiku-4-5": {
    "id": "claude-haiku-4-5",
    "openrouterId": "anthropic/claude-haiku-4.5",
    "displayName": "Claude Haiku 4.5",
    "provider": "Anthropic",
    "description": "Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\n\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring >73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.",
    "contextLength": 200000,
    "cost": {
      "prompt": 0.000001,
      "completion": 0.000005,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "image",
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gpt-oss-120b": {
    "id": "gpt-oss-120b",
    "openrouterId": "openai/gpt-oss-120b",
    "displayName": "GPT-OSS-120B",
    "provider": "OpenAI",
    "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
    "contextLength": 131072,
    "cost": {
      "prompt": 3.9e-8,
      "completion": 1.9e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  },
  "gpt-oss-20b": {
    "id": "gpt-oss-20b",
    "openrouterId": "openai/gpt-oss-20b",
    "displayName": "GPT-OSS-20B",
    "provider": "OpenAI",
    "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
    "contextLength": 131072,
    "cost": {
      "prompt": 3e-8,
      "completion": 1.4e-7,
      "currency": "USD",
      "unit": "per token"
    },
    "modalities": {
      "input": [
        "text"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": [
      "structured output",
      "function calling"
    ],
    "architecture": "Unknown",
    "released": 2025
  }
}
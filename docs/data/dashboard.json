{
  "metadata": {
    "generatedAt": "2025-12-30T13:56:43.930Z",
    "totalModels": 16,
    "totalEvaluations": 1,
    "totalTestCases": 8,
    "overallBenchmarkScore": 0.4198828125
  },
  "models": [
    {
      "id": "gpt-5-2",
      "displayName": "GPT-5.2",
      "provider": "OpenAI",
      "overallScore": 0.46375,
      "categoryScores": {
        "scripture": 0.46375,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.46375
      }
    },
    {
      "id": "gpt-5-nano",
      "displayName": "GPT-5 Nano",
      "provider": "OpenAI",
      "overallScore": 0.454375,
      "categoryScores": {
        "scripture": 0.454375,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.454375
      }
    },
    {
      "id": "claude-sonnet-4-5",
      "displayName": "Claude Sonnet 4.5",
      "provider": "Anthropic",
      "overallScore": 0.44875,
      "categoryScores": {
        "scripture": 0.44875,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.44875
      }
    },
    {
      "id": "grok-4",
      "displayName": "Grok 4",
      "provider": "X.AI",
      "overallScore": 0.44,
      "categoryScores": {
        "scripture": 0.44,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.44
      }
    },
    {
      "id": "mistral-large-2512",
      "displayName": "Mistral Large 2512",
      "provider": "Mistral",
      "overallScore": 0.439375,
      "categoryScores": {
        "scripture": 0.439375,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.439375
      }
    },
    {
      "id": "claude-opus-4-5",
      "displayName": "Claude Opus 4.5",
      "provider": "Anthropic",
      "overallScore": 0.43687499999999996,
      "categoryScores": {
        "scripture": 0.43687499999999996,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.43687499999999996
      }
    },
    {
      "id": "deepseek-v3-2",
      "displayName": "DeepSeek V3.2",
      "provider": "DeepSeek",
      "overallScore": 0.429375,
      "categoryScores": {
        "scripture": 0.429375,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.429375
      }
    },
    {
      "id": "claude-haiku-4-5",
      "displayName": "Claude Haiku 4.5",
      "provider": "Anthropic",
      "overallScore": 0.428125,
      "categoryScores": {
        "scripture": 0.428125,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.428125
      }
    },
    {
      "id": "intellect-3",
      "displayName": "Intellect-3",
      "provider": "Prime Intellect",
      "overallScore": 0.423125,
      "categoryScores": {
        "scripture": 0.423125,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.423125
      }
    },
    {
      "id": "glm-4-7",
      "displayName": "GLM-4.7",
      "provider": "Zhipu AI",
      "overallScore": 0.419375,
      "categoryScores": {
        "scripture": 0.419375,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.419375
      }
    },
    {
      "id": "minimax-m2-1",
      "displayName": "MiniMax M2.1",
      "provider": "MiniMax",
      "overallScore": 0.41562499999999997,
      "categoryScores": {
        "scripture": 0.41562499999999997,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.41562499999999997
      }
    },
    {
      "id": "grok-4-1-fast",
      "displayName": "Grok 4.1 Fast",
      "provider": "X.AI",
      "overallScore": 0.41187500000000005,
      "categoryScores": {
        "scripture": 0.41187500000000005,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.41187500000000005
      }
    },
    {
      "id": "gemini-3-flash-preview",
      "displayName": "Gemini 3 Flash Preview",
      "provider": "Google",
      "overallScore": 0.41187499999999994,
      "categoryScores": {
        "scripture": 0.41187499999999994,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.41187499999999994
      }
    },
    {
      "id": "gemini-3-pro-preview",
      "displayName": "Gemini 3 Pro Preview",
      "provider": "Google",
      "overallScore": 0.4025,
      "categoryScores": {
        "scripture": 0.4025,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.4025
      }
    },
    {
      "id": "gpt-oss-120b",
      "displayName": "GPT-OSS-120B",
      "provider": "OpenAI",
      "overallScore": 0.37625000000000003,
      "categoryScores": {
        "scripture": 0.37625000000000003,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.37625000000000003
      }
    },
    {
      "id": "gpt-oss-20b",
      "displayName": "GPT-OSS-20B",
      "provider": "OpenAI",
      "overallScore": 0.31687499999999996,
      "categoryScores": {
        "scripture": 0.31687499999999996,
        "theology": null
      },
      "evaluationScores": {
        "Context Understanding": 0.31687499999999996
      }
    }
  ],
  "evaluations": [
    {
      "id": "context-understanding",
      "name": "Context Understanding",
      "category": "scripture",
      "modelScores": {
        "claude-haiku-4-5": 0.428125,
        "claude-opus-4-5": 0.43687499999999996,
        "claude-sonnet-4-5": 0.44875,
        "deepseek-v3-2": 0.429375,
        "gemini-3-flash-preview": 0.41187499999999994,
        "gemini-3-pro-preview": 0.4025,
        "glm-4-7": 0.419375,
        "gpt-5-nano": 0.454375,
        "gpt-5-2": 0.46375,
        "gpt-oss-120b": 0.37625000000000003,
        "gpt-oss-20b": 0.31687499999999996,
        "grok-4-1-fast": 0.41187500000000005,
        "grok-4": 0.44,
        "intellect-3": 0.423125,
        "minimax-m2-1": 0.41562499999999997,
        "mistral-large-2512": 0.439375
      },
      "description": "Tests understanding of biblical context, authorship, and historical background.",
      "methodology": "Questions about authorship, audience, purpose, and historical context of biblical books. Uses LLM-as-judge for nuanced evaluation.",
      "testCases": 8,
      "interpretation": "Higher scores indicate deeper comprehension beyond mere recitation. Tests contextual and historical knowledge."
    }
  ],
  "categories": {
    "scripture": {
      "name": "Scripture Accuracy",
      "description": "Tests foundational knowledge of the Bible itselfâ€”verse recall, reference knowledge, and biblical context.",
      "evaluations": [
        "context-understanding"
      ]
    },
    "theology": {
      "name": "Theological Understanding",
      "description": "Tests comprehension of Christian doctrine, orthodoxy, denominational diversity, and pastoral application.",
      "evaluations": []
    }
  }
}
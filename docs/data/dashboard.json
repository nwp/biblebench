{
  "metadata": {
    "generatedAt": "2025-12-31T17:50:12.088Z",
    "totalModels": 19,
    "totalEvaluations": 11,
    "totalTestCases": 149,
    "overallBenchmarkScore": 0.6081604377723303
  },
  "models": [
    {
      "id": "grok-4-1-fast",
      "displayName": "Grok 4.1 Fast",
      "provider": "X.AI",
      "overallScore": 0.8506921332427165,
      "categoryScores": {
        "scripture": 0.8731944444444445,
        "theology": 0.8410482855848331
      },
      "evaluationScores": {
        "Context Understanding": 0.8487500000000001,
        "Core Doctrines": 0.8393622144617977,
        "Denominational Nuance": 0.9083333333333332,
        "Exact Scripture Matching": 0.9375,
        "Heresy Detection": 0.9685714285714284,
        "Pastoral Application": 0.9085000000000001,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.8898437499999998,
        "Steering Compliance - Conservative": 0.6613636363636364,
        "Steering Compliance - Progressive": 0.7113636363636364,
        "Theological Orientation": 0.6523809523809523
      }
    },
    {
      "id": "mistral-large-2512",
      "displayName": "Mistral Large 2512",
      "provider": "Mistral",
      "overallScore": 0.838114803732158,
      "categoryScores": {
        "scripture": 0.9285416666666667,
        "theology": 0.7993604339030831
      },
      "evaluationScores": {
        "Context Understanding": 0.923125,
        "Core Doctrines": 0.9006936271484199,
        "Denominational Nuance": 0.9175000000000001,
        "Exact Scripture Matching": 0.8958333333333334,
        "Heresy Detection": 0.9738095238095239,
        "Pastoral Application": 0.8969999999999999,
        "Reference Knowledge": 0.9666666666666667,
        "Sect Theology": 0.9326562500000001,
        "Steering Compliance - Conservative": 0.4897727272727273,
        "Steering Compliance - Progressive": 0.4840909090909091,
        "Theological Orientation": 0.5499999999999999
      }
    },
    {
      "id": "claude-sonnet-4-5",
      "displayName": "Claude Sonnet 4.5",
      "provider": "Anthropic",
      "overallScore": 0.8360819840254712,
      "categoryScores": {
        "scripture": 0.9071527777777777,
        "theology": 0.8056230724173395
      },
      "evaluationScores": {
        "Context Understanding": 0.8631249999999999,
        "Core Doctrines": 0.8288942450166145,
        "Denominational Nuance": 0.9166666666666669,
        "Exact Scripture Matching": 0.9583333333333334,
        "Heresy Detection": 0.9652380952380952,
        "Pastoral Application": 0.9419999999999998,
        "Reference Knowledge": 0.9,
        "Sect Theology": 0.8990625000000001,
        "Steering Compliance - Conservative": 0.5329545454545456,
        "Steering Compliance - Progressive": 0.5545454545454546,
        "Theological Orientation": 0.5174603174603176
      }
    },
    {
      "id": "gpt-5-nano",
      "displayName": "GPT-5 Nano",
      "provider": "OpenAI",
      "overallScore": 0.8327566130220985,
      "categoryScores": {
        "scripture": 0.867638888888889,
        "theology": 0.8178070662220455
      },
      "evaluationScores": {
        "Context Understanding": 0.91125,
        "Core Doctrines": 0.8545423206971742,
        "Denominational Nuance": 0.9558333333333334,
        "Exact Scripture Matching": 0.7916666666666666,
        "Heresy Detection": 0.9695238095238097,
        "Pastoral Application": 0.891,
        "Reference Knowledge": 0.9,
        "Sect Theology": 0.9662500000000002,
        "Steering Compliance - Conservative": 0.4556818181818183,
        "Steering Compliance - Progressive": 0.6318181818181818,
        "Theological Orientation": 0.557142857142857
      }
    },
    {
      "id": "claude-opus-4-5",
      "displayName": "Claude Opus 4.5",
      "provider": "Anthropic",
      "overallScore": 0.8320850666705791,
      "categoryScores": {
        "scripture": 0.9090972222222221,
        "theology": 0.7990798571484463
      },
      "evaluationScores": {
        "Context Understanding": 0.8856249999999999,
        "Core Doctrines": 0.823487796936671,
        "Denominational Nuance": 0.9404166666666666,
        "Exact Scripture Matching": 0.875,
        "Heresy Detection": 0.9382539682539682,
        "Pastoral Application": 0.9415000000000001,
        "Reference Knowledge": 0.9666666666666667,
        "Sect Theology": 0.8817187500000001,
        "Steering Compliance - Conservative": 0.5545454545454546,
        "Steering Compliance - Progressive": 0.5136363636363637,
        "Theological Orientation": 0.5222222222222224
      }
    },
    {
      "id": "grok-4",
      "displayName": "Grok 4",
      "provider": "X.AI",
      "overallScore": 0.8266467405021899,
      "categoryScores": {
        "scripture": 0.91375,
        "theology": 0.7893167721459856
      },
      "evaluationScores": {
        "Context Understanding": 0.9037499999999999,
        "Core Doctrines": 0.8519981950651885,
        "Denominational Nuance": 0.9245833333333334,
        "Exact Scripture Matching": 0.9375,
        "Heresy Detection": 0.9657142857142857,
        "Pastoral Application": 0.9567,
        "Reference Knowledge": 0.9,
        "Sect Theology": 0.8728125,
        "Steering Compliance - Conservative": 0.47727272727272735,
        "Steering Compliance - Progressive": 0.4761363636363637,
        "Theological Orientation": 0.49444444444444446
      }
    },
    {
      "id": "deepseek-v3-2",
      "displayName": "DeepSeek V3.2",
      "provider": "DeepSeek",
      "overallScore": 0.8258628610200326,
      "categoryScores": {
        "scripture": 0.9134027777777778,
        "theology": 0.7883457538381416
      },
      "evaluationScores": {
        "Context Understanding": 0.881875,
        "Core Doctrines": 0.84660650522197,
        "Denominational Nuance": 0.9333333333333332,
        "Exact Scripture Matching": 0.9583333333333334,
        "Heresy Detection": 0.9728571428571429,
        "Pastoral Application": 0.8672000000000001,
        "Reference Knowledge": 0.9,
        "Sect Theology": 0.9279687499999999,
        "Steering Compliance - Conservative": 0.5,
        "Steering Compliance - Progressive": 0.4704545454545455,
        "Theological Orientation": 0.5547619047619047
      }
    },
    {
      "id": "gpt-5-2",
      "displayName": "GPT-5.2",
      "provider": "OpenAI",
      "overallScore": 0.8165568595117554,
      "categoryScores": {
        "scripture": 0.8493055555555555,
        "theology": 0.8025217040644124
      },
      "evaluationScores": {
        "Context Understanding": 0.88125,
        "Core Doctrines": 0.8772548324912895,
        "Denominational Nuance": 0.9329166666666668,
        "Exact Scripture Matching": 0.8333333333333334,
        "Heresy Detection": 0.9511111111111111,
        "Pastoral Application": 0.9434999999999999,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.9446875000000003,
        "Steering Compliance - Conservative": 0.4625,
        "Steering Compliance - Progressive": 0.5056818181818182,
        "Theological Orientation": 0.5714285714285714
      }
    },
    {
      "id": "glm-4-7",
      "displayName": "GLM-4.7",
      "provider": "Zhipu AI",
      "overallScore": 0.8149498403438112,
      "categoryScores": {
        "scripture": 0.8872222222222222,
        "theology": 0.7839759623959209
      },
      "evaluationScores": {
        "Context Understanding": 0.87,
        "Core Doctrines": 0.8711204272476376,
        "Denominational Nuance": 0.9283333333333333,
        "Exact Scripture Matching": 0.9583333333333334,
        "Heresy Detection": 0.9661904761904762,
        "Pastoral Application": 0.9024999999999999,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.8946875000000001,
        "Steering Compliance - Conservative": 0.45454545454545453,
        "Steering Compliance - Progressive": 0.4704545454545455,
        "Theological Orientation": 0.5666666666666667
      }
    },
    {
      "id": "gemini-3-pro-preview",
      "displayName": "Gemini 3 Pro Preview",
      "provider": "Google",
      "overallScore": 0.8100270509940743,
      "categoryScores": {
        "scripture": 0.9145833333333333,
        "theology": 0.7652172157058205
      },
      "evaluationScores": {
        "Context Understanding": 0.90625,
        "Core Doctrines": 0.8233857696810031,
        "Denominational Nuance": 0.9129166666666668,
        "Exact Scripture Matching": 0.9375,
        "Heresy Detection": 0.959047619047619,
        "Pastoral Application": 0.8835000000000001,
        "Reference Knowledge": 0.9,
        "Sect Theology": 0.8981250000000001,
        "Steering Compliance - Conservative": 0.4318181818181819,
        "Steering Compliance - Progressive": 0.4477272727272728,
        "Theological Orientation": 0.5722222222222223
      }
    },
    {
      "id": "gpt-5-mini",
      "displayName": "GPT-5 Mini",
      "provider": "OpenAI",
      "overallScore": 0.8059854779926399,
      "categoryScores": {
        "scripture": 0.7802083333333334,
        "theology": 0.8170328257037713
      },
      "evaluationScores": {
        "Context Understanding": 0.9031250000000002,
        "Core Doctrines": 0.8385650071991265,
        "Denominational Nuance": 0.8983333333333334,
        "Exact Scripture Matching": 0.6041666666666666,
        "Heresy Detection": 0.9766666666666668,
        "Pastoral Application": 0.882,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.9259375000000003,
        "Steering Compliance - Conservative": 0.6318181818181818,
        "Steering Compliance - Progressive": 0.5659090909090909,
        "Theological Orientation": 0.5611111111111111
      }
    },
    {
      "id": "gemini-3-flash-preview",
      "displayName": "Gemini 3 Flash Preview",
      "provider": "Google",
      "overallScore": 0.7912445423847123,
      "categoryScores": {
        "scripture": 0.8646527777777777,
        "theology": 0.7597838700733986
      },
      "evaluationScores": {
        "Context Understanding": 0.8231249999999999,
        "Core Doctrines": 0.8268969931111929,
        "Denominational Nuance": 0.915,
        "Exact Scripture Matching": 0.9375,
        "Heresy Detection": 0.9628571428571429,
        "Pastoral Application": 0.8644999999999999,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.8696875000000001,
        "Steering Compliance - Conservative": 0.4375,
        "Steering Compliance - Progressive": 0.44204545454545463,
        "Theological Orientation": 0.5626984126984127
      }
    },
    {
      "id": "minimax-m2-1",
      "displayName": "MiniMax M2.1",
      "provider": "MiniMax",
      "overallScore": 0.7893009241209022,
      "categoryScores": {
        "scripture": 0.7867361111111112,
        "theology": 0.7904001296965272
      },
      "evaluationScores": {
        "Context Understanding": 0.8768750000000001,
        "Core Doctrines": 0.8870551015986332,
        "Denominational Nuance": 0.9279166666666666,
        "Exact Scripture Matching": 0.6666666666666666,
        "Heresy Detection": 0.9614285714285716,
        "Pastoral Application": 0.8739999999999999,
        "Reference Knowledge": 0.8166666666666667,
        "Sect Theology": 0.8892187500000001,
        "Steering Compliance - Conservative": 0.46136363636363636,
        "Steering Compliance - Progressive": 0.5318181818181819,
        "Theological Orientation": 0.5261904761904762
      }
    },
    {
      "id": "intellect-3",
      "displayName": "Intellect-3",
      "provider": "Prime Intellect",
      "overallScore": 0.7888564902632,
      "categoryScores": {
        "scripture": 0.80375,
        "theology": 0.7824735575188573
      },
      "evaluationScores": {
        "Context Understanding": 0.8987499999999999,
        "Core Doctrines": 0.8855003734112218,
        "Denominational Nuance": 0.8916666666666667,
        "Exact Scripture Matching": 0.7291666666666666,
        "Heresy Detection": 0.9761904761904762,
        "Pastoral Application": 0.844,
        "Reference Knowledge": 0.7833333333333333,
        "Sect Theology": 0.88109375,
        "Steering Compliance - Conservative": 0.48750000000000004,
        "Steering Compliance - Progressive": 0.5113636363636365,
        "Theological Orientation": 0.503968253968254
      }
    },
    {
      "id": "llama-4-maverick",
      "displayName": "Llama 4 Maverick",
      "provider": "Meta",
      "overallScore": 0.7888474415165458,
      "categoryScores": {
        "scripture": 0.834861111111111,
        "theology": 0.7691272974045894
      },
      "evaluationScores": {
        "Context Understanding": 0.8587499999999999,
        "Core Doctrines": 0.8423973047758496,
        "Denominational Nuance": 0.9104166666666668,
        "Exact Scripture Matching": 0.8125,
        "Heresy Detection": 0.9685714285714285,
        "Pastoral Application": 0.8335000000000001,
        "Reference Knowledge": 0.8333333333333334,
        "Sect Theology": 0.8846875000000001,
        "Steering Compliance - Conservative": 0.5386363636363636,
        "Steering Compliance - Progressive": 0.4056818181818182,
        "Theological Orientation": 0.5182539682539682
      }
    },
    {
      "id": "gpt-5-2-pro",
      "displayName": "GPT-5.2 Pro",
      "provider": "OpenAI",
      "overallScore": 0.7789564671573236,
      "categoryScores": {
        "scripture": 0.7255555555555556,
        "theology": 0.8018425721295098
      },
      "evaluationScores": {
        "Context Understanding": 0.91,
        "Core Doctrines": 0.8698667549065693,
        "Denominational Nuance": 0.9395833333333333,
        "Exact Scripture Matching": 0.5,
        "Heresy Detection": 0.9766666666666668,
        "Pastoral Application": 0.8234999999999999,
        "Reference Knowledge": 0.7666666666666667,
        "Sect Theology": 0.94078125,
        "Steering Compliance - Conservative": 0.5227272727272728,
        "Steering Compliance - Progressive": 0.5397727272727273,
        "Theological Orientation": 0.5825396825396826
      }
    },
    {
      "id": "claude-haiku-4-5",
      "displayName": "Claude Haiku 4.5",
      "provider": "Anthropic",
      "overallScore": 0.7469852921381218,
      "categoryScores": {
        "scripture": 0.6945833333333334,
        "theology": 0.7694432744830311
      },
      "evaluationScores": {
        "Context Understanding": 0.82125,
        "Core Doctrines": 0.7501116244836713,
        "Denominational Nuance": 0.88625,
        "Exact Scripture Matching": 0.3125,
        "Heresy Detection": 0.935079365079365,
        "Pastoral Application": 0.8780000000000001,
        "Reference Knowledge": 0.95,
        "Sect Theology": 0.8298437499999999,
        "Steering Compliance - Conservative": 0.5420454545454546,
        "Steering Compliance - Progressive": 0.5647727272727273,
        "Theological Orientation": 0.4880952380952382
      }
    },
    {
      "id": "gpt-oss-120b",
      "displayName": "GPT-OSS-120B",
      "provider": "OpenAI",
      "overallScore": 0.7100972035872419,
      "categoryScores": {
        "scripture": 0.5995138888888889,
        "theology": 0.7574900527436789
      },
      "evaluationScores": {
        "Context Understanding": 0.8693750000000001,
        "Core Doctrines": 0.7221351852230682,
        "Denominational Nuance": 0.9333333333333335,
        "Exact Scripture Matching": 0.22916666666666666,
        "Heresy Detection": 0.9542857142857143,
        "Pastoral Application": 0.7859999999999999,
        "Reference Knowledge": 0.7,
        "Sect Theology": 0.9078125000000001,
        "Steering Compliance - Conservative": 0.4965909090909092,
        "Steering Compliance - Progressive": 0.5022727272727272,
        "Theological Orientation": 0.5642857142857143
      }
    },
    {
      "id": "gpt-oss-20b",
      "displayName": "GPT-OSS-20B",
      "provider": "OpenAI",
      "overallScore": 0.6900507660136318,
      "categoryScores": {
        "scripture": 0.6018055555555556,
        "theology": 0.727870141924236
      },
      "evaluationScores": {
        "Context Understanding": 0.6887500000000001,
        "Core Doctrines": 0.692107497798656,
        "Denominational Nuance": 0.77375,
        "Exact Scripture Matching": 0.3333333333333333,
        "Heresy Detection": 0.8847619047619046,
        "Pastoral Application": 0.7495,
        "Reference Knowledge": 0.7833333333333333,
        "Sect Theology": 0.8540625000000001,
        "Steering Compliance - Conservative": 0.6000000000000001,
        "Steering Compliance - Progressive": 0.5409090909090909,
        "Theological Orientation": 0.5087301587301587
      }
    }
  ],
  "evaluations": [
    {
      "id": "context-understanding",
      "name": "Context Understanding",
      "category": "scripture",
      "modelScores": {
        "claude-haiku-4-5": 0.82125,
        "claude-opus-4-5": 0.8856249999999999,
        "claude-sonnet-4-5": 0.8631249999999999,
        "deepseek-v3-2": 0.881875,
        "gemini-3-flash-preview": 0.8231249999999999,
        "gemini-3-pro-preview": 0.90625,
        "glm-4-7": 0.87,
        "gpt-5-mini": 0.9031250000000002,
        "gpt-5-nano": 0.91125,
        "gpt-5-2": 0.88125,
        "gpt-5-2-pro": 0.91,
        "gpt-oss-120b": 0.8693750000000001,
        "gpt-oss-20b": 0.6887500000000001,
        "grok-4": 0.9037499999999999,
        "grok-4-1-fast": 0.8487500000000001,
        "intellect-3": 0.8987499999999999,
        "llama-4-maverick": 0.8587499999999999,
        "minimax-m2-1": 0.8768750000000001,
        "mistral-large-2512": 0.923125
      },
      "description": "Tests understanding of biblical context, authorship, and historical background.",
      "methodology": "Questions about authorship, audience, purpose, and historical context of biblical books. Uses LLM-as-judge for nuanced evaluation.",
      "testCases": 8,
      "interpretation": "Higher scores indicate deeper comprehension beyond mere recitation. Tests contextual and historical knowledge."
    },
    {
      "id": "core-doctrines",
      "name": "Core Doctrines",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.7501116244836713,
        "claude-opus-4-5": 0.823487796936671,
        "claude-sonnet-4-5": 0.8288942450166145,
        "deepseek-v3-2": 0.84660650522197,
        "gemini-3-flash-preview": 0.8268969931111929,
        "gemini-3-pro-preview": 0.8233857696810031,
        "glm-4-7": 0.8711204272476376,
        "gpt-5-mini": 0.8385650071991265,
        "gpt-5-nano": 0.8545423206971742,
        "gpt-5-2": 0.8772548324912895,
        "gpt-5-2-pro": 0.8698667549065693,
        "gpt-oss-120b": 0.7221351852230682,
        "gpt-oss-20b": 0.692107497798656,
        "grok-4": 0.8519981950651885,
        "grok-4-1-fast": 0.8393622144617977,
        "intellect-3": 0.8855003734112218,
        "llama-4-maverick": 0.8423973047758496,
        "minimax-m2-1": 0.8870551015986332,
        "mistral-large-2512": 0.9006936271484199
      },
      "description": "Tests understanding of foundational Christian theological concepts.",
      "methodology": "Questions about Trinity, Hypostatic Union, Justification, Original Sin, Divine Sovereignty, Imago Dei, Gospel, and Resurrection. Evaluated with theological accuracy judge and completeness scoring.",
      "testCases": 8,
      "interpretation": "Higher scores indicate better understanding of historic Christian orthodoxy. Tests non-negotiable doctrinal foundations."
    },
    {
      "id": "denominational-nuance",
      "name": "Denominational Nuance",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.88625,
        "claude-opus-4-5": 0.9404166666666666,
        "claude-sonnet-4-5": 0.9166666666666669,
        "deepseek-v3-2": 0.9333333333333332,
        "gemini-3-flash-preview": 0.915,
        "gemini-3-pro-preview": 0.9129166666666668,
        "glm-4-7": 0.9283333333333333,
        "gpt-5-mini": 0.8983333333333334,
        "gpt-5-nano": 0.9558333333333334,
        "gpt-5-2": 0.9329166666666668,
        "gpt-5-2-pro": 0.9395833333333333,
        "gpt-oss-120b": 0.9333333333333335,
        "gpt-oss-20b": 0.77375,
        "grok-4": 0.9245833333333334,
        "grok-4-1-fast": 0.9083333333333332,
        "intellect-3": 0.8916666666666667,
        "llama-4-maverick": 0.9104166666666668,
        "minimax-m2-1": 0.9279166666666666,
        "mistral-large-2512": 0.9175000000000001
      },
      "description": "Tests fair representation of theological differences between Christian traditions without bias.",
      "methodology": "Questions about Catholic vs. Protestant, baptism practices, Calvinist vs. Arminian, eschatology, and spiritual gifts. Measured with bias detection and balance scoring.",
      "testCases": 6,
      "interpretation": "Higher scores indicate better ability to represent multiple Christian traditions fairly without denominational bias."
    },
    {
      "id": "exact-scripture-matching",
      "name": "Exact Scripture Matching",
      "category": "scripture",
      "modelScores": {
        "claude-haiku-4-5": 0.3125,
        "claude-opus-4-5": 0.875,
        "claude-sonnet-4-5": 0.9583333333333334,
        "deepseek-v3-2": 0.9583333333333334,
        "gemini-3-flash-preview": 0.9375,
        "gemini-3-pro-preview": 0.9375,
        "glm-4-7": 0.9583333333333334,
        "gpt-5-mini": 0.6041666666666666,
        "gpt-5-nano": 0.7916666666666666,
        "gpt-5-2": 0.8333333333333334,
        "gpt-5-2-pro": 0.5,
        "gpt-oss-120b": 0.22916666666666666,
        "gpt-oss-20b": 0.3333333333333333,
        "grok-4": 0.9375,
        "grok-4-1-fast": 0.9375,
        "intellect-3": 0.7291666666666666,
        "llama-4-maverick": 0.8125,
        "minimax-m2-1": 0.6666666666666666,
        "mistral-large-2512": 0.8958333333333334
      },
      "description": "Tests LLMs' ability to recall Bible verses with exact wording across multiple translations.",
      "methodology": "Precise recall of Bible verses across KJV, NIV, ESV, and NASB translations. Requires perfect matches—every word, comma, and punctuation mark must be correct.",
      "testCases": 48,
      "interpretation": "Higher scores indicate better memorization of scripture text across translations. Perfect score requires exact wording including punctuation."
    },
    {
      "id": "heresy-detection",
      "name": "Heresy Detection",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.935079365079365,
        "claude-opus-4-5": 0.9382539682539682,
        "claude-sonnet-4-5": 0.9652380952380952,
        "deepseek-v3-2": 0.9728571428571429,
        "gemini-3-flash-preview": 0.9628571428571429,
        "gemini-3-pro-preview": 0.959047619047619,
        "glm-4-7": 0.9661904761904762,
        "gpt-5-mini": 0.9766666666666668,
        "gpt-5-nano": 0.9695238095238097,
        "gpt-5-2": 0.9511111111111111,
        "gpt-5-2-pro": 0.9766666666666668,
        "gpt-oss-120b": 0.9542857142857143,
        "gpt-oss-20b": 0.8847619047619046,
        "grok-4": 0.9657142857142857,
        "grok-4-1-fast": 0.9685714285714284,
        "intellect-3": 0.9761904761904762,
        "llama-4-maverick": 0.9685714285714285,
        "minimax-m2-1": 0.9614285714285716,
        "mistral-large-2512": 0.9738095238095239
      },
      "description": "Tests ability to correctly identify heretical teachings vs. orthodox doctrine.",
      "methodology": "Questions about historical heresies (Adoptionism, Modalism, Pelagianism, Docetism, Marcionism, Gnosticism). Tests both identification and explanation.",
      "testCases": 7,
      "interpretation": "Higher scores indicate better ability to distinguish orthodoxy from heterodoxy. Tests theological discernment."
    },
    {
      "id": "pastoral-application",
      "name": "Pastoral Application",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.8780000000000001,
        "claude-opus-4-5": 0.9415000000000001,
        "claude-sonnet-4-5": 0.9419999999999998,
        "deepseek-v3-2": 0.8672000000000001,
        "gemini-3-flash-preview": 0.8644999999999999,
        "gemini-3-pro-preview": 0.8835000000000001,
        "glm-4-7": 0.9024999999999999,
        "gpt-5-mini": 0.882,
        "gpt-5-nano": 0.891,
        "gpt-5-2": 0.9434999999999999,
        "gpt-5-2-pro": 0.8234999999999999,
        "gpt-oss-120b": 0.7859999999999999,
        "gpt-oss-20b": 0.7495,
        "grok-4": 0.9567,
        "grok-4-1-fast": 0.9085000000000001,
        "intellect-3": 0.844,
        "llama-4-maverick": 0.8335000000000001,
        "minimax-m2-1": 0.8739999999999999,
        "mistral-large-2512": 0.8969999999999999
      },
      "description": "Tests ability to apply theology to real-world situations with wisdom, sensitivity, and biblical grounding.",
      "methodology": "Real-world pastoral care scenarios requiring theology + pastoral sensitivity + practical wisdom. Multi-dimensional LLM-as-judge evaluation.",
      "testCases": 5,
      "interpretation": "Higher scores indicate better ability to apply theology pastorally, balancing truth with grace in practical situations."
    },
    {
      "id": "reference-knowledge",
      "name": "Reference Knowledge",
      "category": "scripture",
      "modelScores": {
        "claude-haiku-4-5": 0.95,
        "claude-opus-4-5": 0.9666666666666667,
        "claude-sonnet-4-5": 0.9,
        "deepseek-v3-2": 0.9,
        "gemini-3-flash-preview": 0.8333333333333334,
        "gemini-3-pro-preview": 0.9,
        "glm-4-7": 0.8333333333333334,
        "gpt-5-mini": 0.8333333333333334,
        "gpt-5-nano": 0.9,
        "gpt-5-2": 0.8333333333333334,
        "gpt-5-2-pro": 0.7666666666666667,
        "gpt-oss-120b": 0.7,
        "gpt-oss-20b": 0.7833333333333333,
        "grok-4": 0.9,
        "grok-4-1-fast": 0.8333333333333334,
        "intellect-3": 0.7833333333333333,
        "llama-4-maverick": 0.8333333333333334,
        "minimax-m2-1": 0.8166666666666667,
        "mistral-large-2512": 0.9666666666666667
      },
      "description": "Tests ability to correctly identify where specific verses are found in the Bible.",
      "methodology": "Questions about verse locations, testing knowledge of Bible book/chapter/verse structure and reference formats.",
      "testCases": 10,
      "interpretation": "Higher scores indicate better knowledge of where verses are located in scripture. Tests both famous and obscure passages."
    },
    {
      "id": "sect-theology",
      "name": "Sect Theology",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.8298437499999999,
        "claude-opus-4-5": 0.8817187500000001,
        "claude-sonnet-4-5": 0.8990625000000001,
        "deepseek-v3-2": 0.9279687499999999,
        "gemini-3-flash-preview": 0.8696875000000001,
        "gemini-3-pro-preview": 0.8981250000000001,
        "glm-4-7": 0.8946875000000001,
        "gpt-5-mini": 0.9259375000000003,
        "gpt-5-nano": 0.9662500000000002,
        "gpt-5-2": 0.9446875000000003,
        "gpt-5-2-pro": 0.94078125,
        "gpt-oss-120b": 0.9078125000000001,
        "gpt-oss-20b": 0.8540625000000001,
        "grok-4": 0.8728125,
        "grok-4-1-fast": 0.8898437499999998,
        "intellect-3": 0.88109375,
        "llama-4-maverick": 0.8846875000000001,
        "minimax-m2-1": 0.8892187500000001,
        "mistral-large-2512": 0.9326562500000001
      },
      "description": "Tests ability to identify teachings of groups outside historic Christian orthodoxy while maintaining respect.",
      "methodology": "Questions about Mormonism (LDS), Jehovah's Witnesses, Christian Science, Oneness Pentecostalism, and Unitarian Universalism. Evaluates accuracy and respectful tone.",
      "testCases": 16,
      "interpretation": "Higher scores indicate ability to articulate how sect teachings depart from orthodoxy while maintaining respectful tone."
    },
    {
      "id": "steering-compliance-conservative",
      "name": "Steering Compliance - Conservative",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.5420454545454546,
        "claude-opus-4-5": 0.5545454545454546,
        "claude-sonnet-4-5": 0.5329545454545456,
        "deepseek-v3-2": 0.5,
        "gemini-3-flash-preview": 0.4375,
        "gemini-3-pro-preview": 0.4318181818181819,
        "glm-4-7": 0.45454545454545453,
        "gpt-5-mini": 0.6318181818181818,
        "gpt-5-nano": 0.4556818181818183,
        "gpt-5-2": 0.4625,
        "gpt-5-2-pro": 0.5227272727272728,
        "gpt-oss-120b": 0.4965909090909092,
        "gpt-oss-20b": 0.6000000000000001,
        "grok-4": 0.47727272727272735,
        "grok-4-1-fast": 0.6613636363636364,
        "intellect-3": 0.48750000000000004,
        "llama-4-maverick": 0.5386363636363636,
        "minimax-m2-1": 0.46136363636363636,
        "mistral-large-2512": 0.4897727272727273
      },
      "description": "Tests whether models comply with system prompts adopting conservative theological perspectives.",
      "methodology": "System prompts adopting conservative positions on contested theological issues. Measures whether models comply or resist the steering.",
      "testCases": 10,
      "interpretation": "Higher scores indicate better compliance with conservative theological framing. Low scores may indicate refusal or hedging."
    },
    {
      "id": "steering-compliance-progressive",
      "name": "Steering Compliance - Progressive",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.5647727272727273,
        "claude-opus-4-5": 0.5136363636363637,
        "claude-sonnet-4-5": 0.5545454545454546,
        "deepseek-v3-2": 0.4704545454545455,
        "gemini-3-flash-preview": 0.44204545454545463,
        "gemini-3-pro-preview": 0.4477272727272728,
        "glm-4-7": 0.4704545454545455,
        "gpt-5-mini": 0.5659090909090909,
        "gpt-5-nano": 0.6318181818181818,
        "gpt-5-2": 0.5056818181818182,
        "gpt-5-2-pro": 0.5397727272727273,
        "gpt-oss-120b": 0.5022727272727272,
        "gpt-oss-20b": 0.5409090909090909,
        "grok-4": 0.4761363636363637,
        "grok-4-1-fast": 0.7113636363636364,
        "intellect-3": 0.5113636363636365,
        "llama-4-maverick": 0.4056818181818182,
        "minimax-m2-1": 0.5318181818181819,
        "mistral-large-2512": 0.4840909090909091
      },
      "description": "Tests whether models comply with system prompts adopting progressive theological perspectives.",
      "methodology": "System prompts adopting progressive positions on contested theological issues. Measures whether models comply or resist the steering.",
      "testCases": 10,
      "interpretation": "Higher scores indicate better compliance with progressive theological framing. Low scores may indicate refusal or hedging."
    },
    {
      "id": "theological-orientation",
      "name": "Theological Orientation",
      "category": "theology",
      "modelScores": {
        "claude-haiku-4-5": 0.4880952380952382,
        "claude-opus-4-5": 0.5222222222222224,
        "claude-sonnet-4-5": 0.5174603174603176,
        "deepseek-v3-2": 0.5547619047619047,
        "gemini-3-flash-preview": 0.5626984126984127,
        "gemini-3-pro-preview": 0.5722222222222223,
        "glm-4-7": 0.5666666666666667,
        "gpt-5-mini": 0.5611111111111111,
        "gpt-5-nano": 0.557142857142857,
        "gpt-5-2": 0.5714285714285714,
        "gpt-5-2-pro": 0.5825396825396826,
        "gpt-oss-120b": 0.5642857142857143,
        "gpt-oss-20b": 0.5087301587301587,
        "grok-4": 0.49444444444444446,
        "grok-4-1-fast": 0.6523809523809523,
        "intellect-3": 0.503968253968254,
        "llama-4-maverick": 0.5182539682539682,
        "minimax-m2-1": 0.5261904761904762,
        "mistral-large-2512": 0.5499999999999999
      },
      "description": "Descriptive assessment of where models fall on the theological spectrum (progressive to conservative).",
      "methodology": "Questions across Biblical Authority, Gender & Ministry, Sexual Ethics, Gender Identity, Social Issues, and Ecclesiology. Not pass/fail—measures theological positioning.",
      "testCases": 21,
      "interpretation": "Scores from 0 (progressive) to 1 (conservative). Provides insight into models' theological default positions. Not prescriptive."
    }
  ],
  "categories": {
    "scripture": {
      "name": "Scripture Accuracy",
      "description": "Tests foundational knowledge of the Bible itself—verse recall, reference knowledge, and biblical context.",
      "evaluations": [
        "context-understanding",
        "exact-scripture-matching",
        "reference-knowledge"
      ]
    },
    "theology": {
      "name": "Theological Understanding",
      "description": "Tests comprehension of Christian doctrine, orthodoxy, denominational diversity, and pastoral application.",
      "evaluations": [
        "core-doctrines",
        "denominational-nuance",
        "heresy-detection",
        "pastoral-application",
        "sect-theology",
        "steering-compliance-conservative",
        "steering-compliance-progressive",
        "theological-orientation"
      ]
    }
  }
}